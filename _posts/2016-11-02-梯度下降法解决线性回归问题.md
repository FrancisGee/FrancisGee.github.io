---
layout: post
title: 梯度下降法解决线性回归问题
date: 2016-11-02
tag: [algorithms]
---


目录

* TOC 
{:toc}

>刚开始学习机器学习不久，现在介绍机器学习入门中最基础的问题，用梯度下降法解决二元线性回归问题

## 问题背景

*梯度下降法*是通过对优化目标的**代价函数**中的参数值，用*梯度下降*的方法使其逐步逼近使*代价函数*最小的参数值。


$$
\begin{aligned}
h_{\theta} (x) = \theta_{0} + \theta_{1} x
\end{aligned}
$$


**假设函数**




$$
J( \theta_{0}, \theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}     (         h_{\theta}(    x^{(i)}    ) - y^{(i)}   ) ^2
$$


**代价函数**

repeat until convergence      {

$$
\theta_{j} : =  \theta_{j} -  \alpha  \dfrac {\partial } {\partial\theta _{j}}  J( \theta_{0}, \theta_{1})
$$

}
**梯度下降算法**

假设向量X有m个训练集，有一个特征，向量y有m个对应的结果，参数向量theta有两个元素--theta0和theta1, 其中alpha为学习率（这里暂时不用控制），用*梯度下降算法***同时更新**theta中的两个元素，直至theta收敛，*代价函数*Jval最小。

## 代价函数实现
首先要将所有元素**向量化**，为了表达清晰，计算快捷。X设置为m\*2矩阵，y为m\*1矩阵（列向量）,theta为2\*1矩阵（初始化均为0）.

```matlab

	function J = computeCost(X, y, theta)
	%COMPUTECOST Compute cost for linear regression
	%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
	%   parameter for linear regression to fit the data points in X and y

	% Initialize some useful values
	m = length(y); % number of training examples

	% You need to return the following variables correctly 
	J = 0;
	temp = X*theta - y;    %构建中间矩阵temp用来稍后求内积
	c = 2*m;
	result = (temp')*temp;
	J = result/c;
	% Instructions: Compute the cost of a particular choice of theta
	%               You should set J to the cost.
% =========================================================================

end

```
这里的技巧是用内积运算来求平方运算，并且用向量化的思想，将累加运算融入矩阵中。

## 梯度下降算法实现
首先要将*梯度下降算法*的求偏导操作处理，分别对theta0和theta1求偏导，得到*梯度下降算法*如下：



**Gradient descent algorithms**


repeat until convergence      {
$$
\theta_{0} :=  \theta_{0} -  \alpha \frac{1}{m}  ( h_{\theta}(    x^{(i)}    ) - y^{(i)} )
$$

$$
\theta_{1} :=  \theta_{1} -  \alpha \frac{1}{m}  ( h_{\theta}(    x^{(i)}    ) - y^{(i)} )  \cdot x^{(i)}
$$
}

$$
\textbf{  update $\theta_{0}$ and  $\theta_{1}$ simultaneously }
$$



*梯度下降算法正则化*

```matlab

	function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
	%GRADIENTDESCENT Performs gradient descent to learn theta
	%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
	%   taking num_iters gradient steps with learning rate alpha

	% Initialize some useful values
	m = length(y); % number of training examples
	J_history = zeros(num_iters, 1);
	thetaLen = length(theta);
	tempVal = theta; % Just a temporary val to store theta values.


	for iter = 1:num_iters
 
	    % Instructions: Perform a single gradient step on the parameter vector
	    %               theta. 
	    %
	    % Hint: While debugging, it can be useful to print out the values
	    %       of the cost function (computeCost) and gradient here.
	    %
	    temp = (X*theta-y);
	    
	    for i=1:thetaLen
		tempVal(i,1) = sum(temp.*X(:,i));
	    end
	    
	    theta = theta - (alpha/m)*tempVal;

	    % ============================================================

	    % Save the cost J in every iteration    
	    J_history(iter) = computeCost(X, y, theta);

	end

	end

```
这里的技巧在于内层的for循环的写法，通过theta的长度同时处理多个特征参数值，用sum()函数对应累加运算，用点乘运算处理每个训练集的乘法运算而又不与外围累加运算冲突（主要在于X向量的第一列元素全为1，这样便于向量化）

## 再走一步

二元线性回归最理想的模型可以这样解决，但是还是有很多问题值得去思考。*向量化*的方式解决了多元函数问题，函数中的自变量也可以为矩阵。
另外假设函数不一定最优，如果训练集复杂，可否考虑用*多项式拟合*？如果多项式过拟合的化，是否考虑用*正则化*消除过拟合？*梯度下降法*对于多个特征计算慢，是否考虑回到问题的本质，优化求解*梯度下降*的方法，给出代价函数和起偏导数后用更高阶的计算方式？



