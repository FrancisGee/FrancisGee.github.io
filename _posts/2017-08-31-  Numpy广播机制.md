---

layout: post
title: Numpy广播机制
date: 2017-08-31
tags: [Language]

---

目录

* TOC 
{:toc}


>上周博主将最近很火的Andrew出品的《深度学习》课程刷完了一课，**拿到了证书**。先说体验，课程虽然和以前的机器学习课程有重复，并且很基础。但是对于复习一遍基本神经网络的理论知识和公式推导是有帮助的，第一课基本上是**Build a neural network from scratch**,这个是我对于第一课课程内容的体会。最近网易云课堂也出了阉割版的《深度学习》，不过没有编程作业。私以为编程练习是非常重要的，Andrew的编程练习基本上没有给同学设置障碍，**too much hints**.虽然没有障碍，但是跟着做下来，对于熟悉numpy还有一些基本公式的编程是很有帮助的，虽然作业中的一些函数基本已经被现在高级的框架实现了，不需要自己写，比如tensorflow就实现了很多函数。但是有些基本的概念和语言的特性还是要弄清楚的，这篇博客就是来探讨Numpy的广播机制和一些基本公式的python实现。


## 广播是要解决什么问题

我认为没有凭空出现的技术，都是为了解决某个或者某一类问题。首先numpy出现就是为了解决math库不能很好的处理向量和矩阵的运算问题。那么广播机制的出现是为了解决比如w*x + b(其中w为权重矩阵，b为偏置矩阵)这种运算，虽然矩阵大小不一样，或者说**一个为矩阵，另外一个为向量(行或者列)的运算问题**。这种运算用广播可以节省空间和提高运算效率。

## 广播简单举例

基本运算单元有:Scalar-->Vector--->Matrix--->Tensor，都是符合广播的规则的。

这里用Vector和Matrix的运算举例，便于读者理解。

```python
	a = np.random.randn(3, 3)
	b = np.random.randn(3, 1)

	c = a * b

```
上面python代码的行为就是广播，首先b-->(3,3)，然后矩阵对应元素相乘，注意这个地方不是矩阵乘法，而是element-wise操作，这个后面我们会辨析。

## 基本numpy操作

nunpy的操作基本上就是为了解决深度学习中的公式计算问题，主要的numnpy操作有np.sum(), np.dot(), np.multiply(),np.log()....

接下来博主结合具体的公式辨析一下这些操作的实际应用。

**L1损失函数计算**
+ $L1(\hat{y}, y) = \sum_{i = 0}^m \|y^{(i)} - \hat{y}^{(i)}\|$

```python
 loss = np.sum(abs(yhat - y))

```
**L2损失函数计算**
+ $L2(\hat{y}, y) = \sum_{i = 0}^m (y^{(i)} - \hat{y}^{(i)})^{2}$

```python
 loss = np.dot(yhat - y, yhat - y)
```

$if x = [x1, x2, ...,xn], then np.dot(x, x) = \sum_{j = 0}^{n} x_{j}^{2}$


**交叉煽代价函数计算**
+  $L(a^{(i)}, y^{(i)}) = -y^{(i)}log(a^{(i)}) - (1 - y^{(i)})log(1 - a^{(i)}) \quad
   J = \frac{1}{m} \sum_{i = 1}^{m} L(a^{(i)}, y^{(i)})$


```python
 cost = -1./m * np.sum(Y * log(A) + (1 - Y) * log(1 - A))
```

**激活函数计算**
+ $A = \sigma (w^{T}X + b) = (a^{(0)}, a^{(1)},..., a^{(m-1)},a^{(m)})$

```python
 A = sigmoid(np.dot(w.T,X) + b)
```

**反向传播偏导数计算**

+ $\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^{T} \quad
  \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i = 1}^{m} (a^{(i)} - y^{(i)})$

```python
 dw = 1./m * np.dot(X, (A- Y).T)
 db = 1./m * np.sum(A - Y)
```


上述的例子也只是管中窥豹，这里再次**总结**一下：

1. np.dot(): 这个对于1D向量为点积操作，对于2D矩阵为基本的矩阵乘法操作
2. np.multiply(): 为element-wise的乘法操作，和 x1 * x2的效果
3. np.sum() : 向某个方向的坐标轴求和，axis = 0(col), axis = 1(row)，注意keepdims = True

## 后记
虽然研究深度学习不需要了解很多基础，但是这个还是要掌握的。另外Andrew的课程确实不错，懂了这些内容的朋友可以去论坛上帮忙解决问题，一方面巩固了numpy的知识，另一方面帮助了他人，何乐而不为。






