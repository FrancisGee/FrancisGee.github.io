---

layout: post
title: 神经网络之前向传播算法与反向传播算法
date: 2016-11-12
tags: [algorithms]

---

目录

* TOC 
{:toc}

>学习了神经网络的算法之后，发现这个算法确实很精妙。但反向传播算法不是那么容易实现和理解，所以本文就将神经网络算法的一次完整实现过程做一次描述，其中包括前向传播算法和反向传播算法。
>本文以识别手写数字的多分类神经网络为背景说明，有一个输入层，一个隐藏层，一个输出层。由于输入为5000张20*20像素的灰度图像，所以输入神经元的数量为400（特征值为400个，训练集为5000条），隐藏神经元的数量为25个，输出神经元的数量为10个，对应10个不同的分类(0-9)的输出。

## 神经网络的基本结构

![神经网络的基本结构.png](/images/posts/NN/神经网络基本结构.png)

**神经网络的基本结构**

上图为三层神经网络，输入层有三个神经元，其中$$x_{0}$$为偏置神经元，始终为1；隐藏层有三个神经元，其中$$a^{(2)}_{0}$$为偏置神经元，始终为1；输出层有一个神经元。

### 神经网络中各个元素的意义及其计算方法

![神经网络各个元素的意义及其计算方法.png](/images/posts/NN/神经网络各个元素的意义及其计算方法.png)

**神经网络各个元素的意义及其计算方法**

其中$$a^{(j)}_{i}$$表示第j层的第i个神经元受到的激励，$$\Theta^{(j)}$$表示从层j映射到层j+1的权重控制函数矩阵，对这个矩阵中某个元素的 $$\Theta^{(2)}_{12}$$的解释为，上标2表示这是从第二层神经元作用到第三层神经元的权重，所以这个值影响到$$a^{3}_{1}$$,下标中的第一项1表示，这个权重作用于第三层的第一个神经元（不包含偏置神经元），下标中的第二项2表示，这个权重的作用来自与第二层的第2个神经元的影响（如果为0就表示为第二层偏置神经元的影响）

其中如果第j层有$$s_{j}$$个神经元，第j+1层有$$s_{j+1}$$个神经元，那么权重矩阵$$\Theta^{j}$$的维度为$$s_{j+1} \times (s_{j} + 1)$$

### 神经网络分类器问题

![神经网络分类器问题.png](/images/posts/NN/神经网络识别问题.png)

**神经网络分类器问题**

神经网络分类器问题有两类，分为二元分类（如二元逻辑回归）和多分类问题，本文中要研究的是多分类问题。

多分类问题的处理注意一点，如果有K个输出，那么要将其映射为K行向量，即$$y\in R^{K}$$.

## 神经网络代价函数

![神经网络代价函数.png](/images/posts/NN/神经网络代价函数.png)

**神经网络代价函数**

神经网络的代价函数看起来很复杂，但是要和逻辑回归的代价函数作对吧，主要是引入K个分类的参数，具体公式如下：

$$h_{\Theta} (x) \in R^{K}$$     $$ ( h_{\Theta} (x) )_{i} = i^{th} output $$     

    
$$J(\Theta) =   -\frac{1}{m} \left[  \sum_{i=1}^{m} \sum_{k=1}^{K} y_{k}^{(i)} log( h_{\Theta}(x^{(i)}))_{k} + (1 - y_{k}^{(i)}) log(1 - (h_{\Theta} (x^{(i)}))_{k})  \right]  + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}}  \sum_{j=1}^{s_{l}+1} (\Theta_{ji}^{(l)})^{2}   $$            


## 神经网络梯度计算

![神经网络梯度表示.png](/images/posts/NN/神经网络梯度表示.png)

**神经网络梯度表示**

为了用高级的优化算法计算神经网路代价函数的梯度，我们还需要$$ \dfrac {\partial} {\partial \Theta_{ij}^{l}} $$

## 前向传播算法

![前向传播梯度计算.png](/images/posts/NN/前向传播梯度计算.png)

**前向传播梯度计算**

![前向传播算法向量化实现.png](/images/posts/NN/前向传播算法向量化实现.png)

**前向传播算法向量化实现**

具体描述如下：
给一个训练样本(x,y)

计算流程：

$$
a^{(1)} =x
$$

$$
z^{(2)} = \Theta^{(1)} a^{(1)}
$$

$$
a^{(2)} = g( z^{(2)})  ( add \quad a_{0}^{(2)})
$$

$$
z^{(3)} = \Theta^{(2)} a^{(2)}
$$

$$
a^{(3)} = g( z^{(3)})  ( add \quad a_{0}^{(3)})
$$

$$
z^{(4)} = \Theta^{(3)} a^{(3)}
$$

$$
a^{(4)} = h_{\Theta} (x) = g(z^{(4)})
$$


## 反向传播算法

**反向传播算法**

![反向传播梯度计算.png](/images/posts/NN/反向传播梯度计算.png)

反向传播算法是建立在已经完成前向传播算法的基础上的，其中用误差$$ \delta_{j}^{(l)}$$表示第l层的第j个神经元的误差。有最后一层的误差一直往前反推直至第二层。通过这些误差，就可以计算出代价函数的梯度 $$ \dfrac {\partial} {\partial \Theta_{ij}^{l}} $$

比如对于四层神经网络，每个输出神经元的误差计算如下：

$$
\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}
$$

$$
\delta^{(3)} = (\Theta^{3})^{T} \delta^{4} . \ast g'(z^{(3)})
$$

$$
\delta^{(2)} = (\Theta^{2})^{T} \delta^{3} . \ast g'(z^{(2)})
$$

上述流程能够计算出每层的误差$$\delta^{l}$$，那么对于m个训练集，有如下的流程：

对于训练集$$ { (x^{(1)},y^{(1)}),...., (x^{(m)},y^{(m)})   } $$

设初始值 $$\Delta_{ij}^{(l)} = 0 $$,我们就是要算出这个梯度值

设循环 i= 1 to m (训练集个数)

Set $$a^{(1)} = x^{(i)}$$

用前向传播算法，计算出每层的激励 $$a^{(l)}$$ for l =2,3,...L

用 $$y^{(i)}$$计算 $$\delta^{(L)} = a^{(L)} - y^{(i)}$$

用反向传播计算 $$ \delta^{(L-1)},,,,,,,,\delta^{(2)}$$


$$
\Delta_{ij}^{(l)}: =  \Delta_{ij}^{(l)} + a_{j}^{(l)} \delta_{i}^{(l+1)}
$$      
(这个地方我理解的不太明白)

最后: 

$$
D_{ij}^{(l)} : = \frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{l} \quad if \, j \ne 0
$$

$$
D_{ij}^{(l)} : = \frac{1}{m} \Delta_{ij}^{(l)} \quad  if \, j = 0
$$


所以： $$\dfrac{\partial}{\partial\Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} $$ 
    

### 梯度校验

梯度校验算法的目的是在采用神经网络进行正式学习（添加正则化参数$$\theta$$）之前，验证反向传播算法的正确性，如果不对还要继续调试反向传播算法。

程序调试通过后，加上正则化参数$$\lambda$$进行神经网络的参数优化，得到最终的参数$$\Theta$$


## 流程概述

首先选好你要训练的神经网络模型，然后按以下步骤开始训练：

1. 随机初始化**权重值**，这里的权重指的是如线性回归算法初设的**参数值theta**,只不过这里不能向预先一样设theta为全0，这里必须将权重值随机初设化，否则神经网络算法失效。

2. 实现前向传播算法，给每个神经元通过前向传播算法得到激励值，并且得到最后的预测值。

3. 实现神经网络代价函数的计算代码，得到代价函数$$J（\Theta）$$

4. 实现反向传播算法来计算梯度（偏导数）

  
5. 用梯度校验算法（数值计算的梯度值）去验证反向传播算法梯度计算的正确性，验证正确后，关闭梯度校验算法

6. 加上正则化，用梯度下降和高级的计算梯度的数值算法结合反向传播算法来得到代价函数的最小值，从而得出相应的最优权重值


``` matlab

	function [J grad] = nnCostFunction(nn_params, ...
		                            input_layer_size, ...
		                            hidden_layer_size, ...
		                            num_labels, ...
		                            X, y, lambda)
	%NNCOSTFUNCTION Implements the neural network cost function for a two layer
	%neural network which performs classification
	%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...
	%   X, y, lambda) computes the cost and gradient of the neural network. The
	%   parameters for the neural network are "unrolled" into the vector
	%   nn_params and need to be converted back into the weight matrices. 
	% 
	%   The returned parameter grad should be a "unrolled" vector of the
	%   partial derivatives of the neural network.
	%

	% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices
	% for our 2 layer neural network


	Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
		         hidden_layer_size, (input_layer_size + 1));

	Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
		         num_labels, (hidden_layer_size + 1));

	% Setup some useful variables
	m = size(X, 1);
		 
	% You need to return the following variables correctly 
	J = 0;
	Theta1_grad = zeros(size(Theta1));
	Theta2_grad = zeros(size(Theta2));

	% ====================== YOUR CODE HERE ======================
	% Instructions: You should complete the code by working through the
	%               following parts.
	%
	% Part 1: Feedforward the neural network and return the cost in the
	%         variable J. After implementing Part 1, you can verify that your
	%         cost function computation is correct by verifying the cost
	%         computed in ex4.m
	%

	% Part 2: Implement the backpropagation algorithm to compute the gradients
	%         Theta1_grad and Theta2_grad. You should return the partial derivatives of
	%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and
	%         Theta2_grad, respectively. After implementing Part 2, you can check
	%         that your implementation is correct by running checkNNGradients
	%
	%         Note: The vector y passed into the function is a vector of labels
	%               containing values from 1..K. You need to map this vector into a 
	%               binary vector of 1's and 0's to be used with the neural network
	%               cost function.
	%
	%         Hint: We recommend implementing backpropagation using a for-loop
	%               over the training examples if you are implementing it for the 
	%               first time.
	%
	% Part 3: Implement regularization with the cost function and gradients.
	%
	%         Hint: You can implement this around the code for
	%               backpropagation. That is, you can compute the gradients for
	%               the regularization separately and then add them to Theta1_grad
	%               and Theta2_grad from Part 2.
	%

	% 1. Feed-forward to compute h =a3

	% implement forward propagation, similar to ex3

	% Add ones to the X data matrix
		                                                                                                                                    
	X = [ones(m, 1) X];
	%implement forward propagation, similar to ex3

	z1 = sigmoid(Theta1 * X');
	a2 = [ones(1, size(z1, 2)); z1];
	a3 = sigmoid(Theta2 * a2);
	h = a3;

	% Now we tranform the y result vector into a matrix where 1s in the
	% columns map to the corresponding values of y

	yMatrix = zeros(num_labels,m);

	for i=1:num_labels,
	    yMatrix(i,:) = (y==i);
	end


	% Now that we have y as a 10x4000 matrix instead of a 4000x1 vector,
	% we can use it to calculate our cost as compared to h (which is a3)

	% Note that for this vectorized implementation, y(i)k is given as
	% yMatrix and h is given as h(thetha)(x(i))k

	J = (sum( sum( -1*yMatrix.*log(h) - (1 - yMatrix).*log(1-h) ) ))/m;


	%regularition
	tempTheta1 = Theta1(:,2:end);
	tempTheta2 = Theta2(:,2:end);
	reg = (sum(sum(tempTheta1.^2))+sum(sum(tempTheta2.^2)))*lambda./(2*m);

	J = J+reg;

	% Part 2: Implement the backpropagation algorithm
	for k =1:m,
	    % First, we do forward propogation on an X that already contains
	    % the bias node (from above)

	    a1 = X(k,:);
	    z2 = Theta1 * a1';

	    a2 = sigmoid(z2);
	    a2 = [1 ; a2];

	    % Now we have our final activation layer a3 == h(theta)
	    a3 = sigmoid(Theta2 * a2);

	    % Now that we have our activation layer, we go backwards
	    % This basically just involves following along the formulas given
	    % on Page 9
	    d3 = a3 - yMatrix(:,k);
	    
	    % Re-add a bais node for z2
	    z2 = [1 ; z2];
	    d2 = (Theta2' * d3) .* sigmoidGradient(z2);
	    % Strip out bais node from resulting d2
	    d2 = d2(2:end);

	    Theta2_grad = (Theta2_grad + d3 * a2');
	    Theta1_grad = (Theta1_grad + d2 * a1);

	end

	% Now divide everything (element-wise) by m to return the partial
	% derivatives. Note that for regularization these will have to
	% removed/commented out.

	%Theta2_grad = Theta2_grad ./ m;
	%Theta1_grad = Theta1_grad ./ m;

	% Implement for Theta1 and Theta2 when l = 0
	Theta1_grad(:,1) = Theta1_grad(:,1)./m;
	Theta2_grad(:,1) = Theta2_grad(:,1)./m;

	% Implement for Theta1 and Theta 2 when l > 0
	Theta1_grad(:,2:end) = Theta1_grad(:,2:end)./m + ( (lambda/m) * Theta1(:,2:end) );
	Theta2_grad(:,2:end) = Theta2_grad(:,2:end)./m + ( (lambda/m) * Theta2(:,2:end) );
	      
	  
	% -------------------------------------------------------------

	% =========================================================================

	% Unroll gradients
	grad = [Theta1_grad(:) ; Theta2_grad(:)];


	%end


```

## 参考
* [如何简单形象又有趣的讲解神经网络是什么](https://www.zhihu.com/question/22553761)
























