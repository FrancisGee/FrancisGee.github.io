---

layout: post
title: 推荐系统技术
date: 2017-06-24
tag: [projects]

---


目录

* TOC 
{:toc}


## 公式推导

**已知评分矩阵$R(M \times N)$**,其中M表示用户的个数，N表示电影的个数。这里用的是矩阵分解的技术，我们要将矩阵R分解为矩阵$P(M \times K)$和矩阵$Q(K \times N)$，其中K为用户和电影隐式交互信息特征向量的个数。在这个问题中，我们要**求解得到矩阵P和矩阵Q**。因为$P \times Q \approx R$。

定义误差函数为$e_{u,i} = r_{u,i} - \hat{r_{u,i}}$，其中u表示某个用户(user)，i表示某个电影(item),$\hat{r_{u,i}}$表示某个用户对某部电影的评分，$e_{u,i}$表示预测的评分和真实的评分之差，其中$\hat{r_{u,i}} = \sum_{k=1}^{K}p_{uk}q_{ki}$
定义损失函数为SSE = $\sum_{u,i} e_{u,i}^{2}$，即所有用户对电影评分的误差的平方总和。我们的目的是让损失函数最小，从而得到用户特征矩阵P和电影特征矩阵Q。
为了防止过拟合，需要引入正则项,由于P和Q中所有项都是未知参数，所以都加入正则项，并且采用**L2范数**。将损失函数SSE改写为SSE = $\sum_{u,i} e_{u,i}^{2} + \frac{1}{2} \beta \sum_{u} \|p_{u}\|^{2} + \frac{1}{2} \beta \sum_{i} \|q_{i}\|^{2}$。可以进一步将SSE转化为
$$SSE = \sum_{u,i} e_{u,i}^{2} +  \frac{1}{2} \beta \sum_{u} \sum_{k=1}^{K} \|p_{uk}\|^{2} + \frac{1}{2} \beta \sum_{i} \sum_{k=1}^{K} \|q_{ki}\|^{2}$$

这里为了求SSE的最小值，采用**随机梯度下降**的局部**求最优化算法**。

首先求损失函数SSE在矩阵P(u,k)点处的梯度(导数):
$$\frac{\partial}{\partial p_{uk}}SSE = -2(r_{u,i} - \hat{r_{u,i}}) \frac{\partial}{\partial p_{uk}} \hat{r_{u,i}} + \beta p_{uk} = -2e_{u,i}q_{k,i} + \beta p_{uk} $$
同理可得损失函数SSE在矩阵Q(k,i)点处的梯度(导数):
$$ \frac{\partial}{\partial q_{ki}}SSE = -2e_{u,i}p_{u,k} + \beta q_{ki} $$

到这个地方，核心公式推导就讲完了，下面就是用随机梯度下降的方法迭代求解最优解，按**负梯度方向**更新参数，算法的细节在算法实现中有所阐述。






## 算法实现
```python

def initialize(R, K):
    """
    Returns initial matrices for an N X M matrix, R and K features.

    :param R: the matrix to be factorized
    :param K: the number of latent features

    :returns: P, Q initial matrices of N x K and M x K sizes
    """
    N, M = R.shape
    P = np.random.rand(N,K)
    Q = np.random.rand(M,K)

    return P, Q

def factor(R, P=None, Q=None, K=2, steps=5000, alpha=0.0002, beta=0.02):
    """
    Performs matrix factorization on R with given parameters.

    :param R: A matrix to be factorized, dimension N x M
    :param P: an initial matrix of dimension N x K
    :param Q: an initial matrix of dimension M x K
    :param K: the number of latent features
    :param steps: the maximum number of iterations to optimize in
    :param alpha: the learning rate for gradient descent
    :param beta:  the regularization parameter

    :returns: final matrices P and Q
    """

    if not P or not Q:
        P, Q = initialize(R, K)
    Q = Q.T

    rows, cols = R.shape
    for step in xrange(steps):
        for i in xrange(rows):
            for j in xrange(cols):
                if R[i,j] > 0:
                    eij = R[i,j] - np.dot(P[i,:], Q[:,j])
                    for k in xrange(K):
                        P[i,k] = P[i,k] + alpha * (2 * eij * Q[k,j] - beta * P[i,k])
                        Q[k,j] = Q[k,j] + alpha * (2 * eij * P[i,k] - beta * Q[k,j])

        e  = 0
        for i in xrange(rows):
            for j in xrange(cols):
                if R[i,j] > 0:
                    e = e + pow(R[i,j] - np.dot(P[i,:], Q[:,j]), 2)
                    for k in xrange(K):
                        e = e + (beta/2) * (pow(P[i,k], 2) + pow(Q[k,j], 2))
        if e < 0.001:
            break

    return P, Q.T
```
实现细节：

1.每次在负梯度方向更新参数P和参数Q。

2.随机梯度下降的方式，因为SGD是每个样本更新后就更新参数，而BGS是所有样本更新完了再更新。

3.在每一次迭代中，利用每个样本更新参数P和参数Q(随机梯度下降)，然后计算所有电影预测评分的损失函数，如果迭代次数未到或者损失函数未到阈值，则进行下一次迭代。
